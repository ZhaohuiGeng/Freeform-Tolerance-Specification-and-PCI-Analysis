{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from scipy import linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from itertools import combinations\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ball = pd.read_csv('Half Ball Prediction Data.csv')\n",
    "convex = pd.read_csv('Convex Prediction Data.csv')\n",
    "freeform = pd.read_csv('Freeform 2 Prediction Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperbolic(x):\n",
    "    return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "\n",
    "def generate_neurons_uniform(X,num_neurons):\n",
    "    k = X.shape[1]\n",
    "    variances = np.std(X,axis=0)\n",
    "    weights = []\n",
    "    for i in range(k):\n",
    "        λ = variances[i]/np.sum(variances)\n",
    "        a = 2.5 * λ / np.max(np.abs(X[:,i]))\n",
    "        weight = np.random.uniform(low=-a,high=a,size=num_neurons)\n",
    "        weights.append(weight)\n",
    "    weights = np.array(weights)\n",
    "    intercept = np.random.uniform(low=-1,high=1,size=num_neurons)\n",
    "    \n",
    "    return weights, intercept\n",
    "\n",
    "def cal_neuron_val(X, weights, intercept):\n",
    "    neuron_val = np.matmul(X,weights) + intercept\n",
    "    neuron_val = hyperbolic(neuron_val)\n",
    "    return neuron_val\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbelm(data,coordinate,num_neurons=50):\n",
    "    weights, intercept = generate_neurons_uniform(data[['Mean Curvature','Mean Angle']].values,num_neurons=num_neurons)\n",
    "    neuron_val = cal_neuron_val(data[['Mean Curvature','Mean Angle']].values,weights,intercept)\n",
    "    clf = BayesianRidge()\n",
    "    name = 'Pointwise Variance ' + coordinate\n",
    "    clf.fit(neuron_val, data[name].values)\n",
    "    return {'weight':weights,'intercept':intercept,'model':clf}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbelm_pred(train,model):\n",
    "    neuron_pred = cal_neuron_val(train[['curvature','angle']].values, model['weight'],model['intercept'])\n",
    "    return model['model'].predict(neuron_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "convex_x = mbelm(convex,'X')\n",
    "convex_y = mbelm(convex,'Y')\n",
    "convex_z = mbelm(convex,'Z')\n",
    "\n",
    "freeform_x = mbelm(freeform,'X')\n",
    "freeform_y = mbelm(freeform,'Y')\n",
    "freeform_z = mbelm(freeform,'Z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procrustes_distance(landmarks, mean):\n",
    "    sum_dis = 0\n",
    "    for i in range(len(landmarks)):\n",
    "        sum_dis += np.linalg.norm(landmarks[i][['X','Y','Z']]-mean) ** 2\n",
    "    return sum_dis\n",
    "\n",
    "def opr(X_1, X_2):\n",
    "    # Ordinary Procrustes registration involve only rotation.\n",
    "    # Rotate X_1 onto X_2\n",
    "    reg_mat = np.matmul(X_2.T, X_1) / (np.linalg.norm(X_1)*np.linalg.norm(X_2))\n",
    "    V, Λ, U_t = np.linalg.svd(reg_mat)\n",
    "    Γ = np.matmul(U_t.T, V.T)\n",
    "    return Γ\n",
    "\n",
    "def opa(X_1,mean):\n",
    "    Γ = opr(X_1[['X','Y','Z']].values, mean[['X','Y','Z']].values)\n",
    "    X_1[['X','Y','Z']] = np.matmul(X_1[['X','Y','Z']], Γ)\n",
    "    return X_1\n",
    "\n",
    "def ppa(landmarks):\n",
    "    # Partial Procrustes registration algorithm for landmarks.\n",
    "    \n",
    "    ## Step 1: Center the landmarks\n",
    "    size = len(landmarks[0])\n",
    "    for i in range(len(landmarks)):\n",
    "        x = np.mean(landmarks[i]['X'])\n",
    "        y = np.mean(landmarks[i]['Y'])\n",
    "        z = np.mean(landmarks[i]['Z'])\n",
    "        landmarks[i][['X', 'Y', 'Z']] = landmarks[i][['X', 'Y', 'Z']] - np.array([x, y, z])\n",
    "\n",
    "        \n",
    "    \n",
    "    ## Step 2: Initialize μ\n",
    "    for i in range(len(landmarks)):\n",
    "        if i == 0:\n",
    "            sum_pts = landmarks[0][['X','Y','Z']]\n",
    "        else:\n",
    "            sum_pts += landmarks[i][['X', 'Y','Z']]\n",
    "            \n",
    "    μ = sum_pts / len(landmarks)\n",
    "    \n",
    "    ## Step 3: Remove rotation\n",
    "    step = 0\n",
    "\n",
    "    test = True\n",
    "    while test:\n",
    "        step+=1\n",
    "#         print(step)\n",
    "        distance = procrustes_distance(landmarks,μ)\n",
    "        \n",
    "        for i in range(len(landmarks)):\n",
    "            Γ = opr(landmarks[i][['X','Y','Z']].values, μ.values)\n",
    "            landmarks[i][['X','Y','Z']] = np.matmul(landmarks[i][['X','Y','Z']], Γ)\n",
    "            landmarks[i][['nX','nY','nZ']] = np.matmul(landmarks[i][['nX','nY','nZ']],Γ)\n",
    "        \n",
    "        for i in range(len(landmarks)):\n",
    "            if i == 0:\n",
    "                sum_pts = landmarks[0][['X','Y','Z']]\n",
    "                sum_normal = landmarks[0][['nX','nY','nZ']]\n",
    "            else:\n",
    "                sum_pts += landmarks[i][['X', 'Y','Z']]\n",
    "                sum_normal += landmarks[i][['nX','nY','nZ']]\n",
    "            \n",
    "        μ = sum_pts / len(landmarks)\n",
    "        μ_normal = sum_normal / len(landmarks)\n",
    "        \n",
    "        temp_dist = procrustes_distance(landmarks, μ)\n",
    "        if (abs(distance - temp_dist) < 0.0001):\n",
    "            test = False\n",
    "    \n",
    "    mean = pd.DataFrame()\n",
    "    mean[['X','Y','Z']] = μ[['X','Y','Z']]\n",
    "    mean[['nX','nY','nZ']] = μ_normal[['nX','nY','nZ']]\n",
    "\n",
    "    return landmarks, mean\n",
    "    \n",
    "def means(lms):\n",
    "    for i in range(len(lms)):\n",
    "        if i == 0:\n",
    "            sum_total = lms[i][['curvature','angle','Z']]\n",
    "        else:\n",
    "            sum_total += lms[i][['curvature','angle','Z']]\n",
    "    return sum_total/len(lms)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### ball ###########\n",
    "landmark = pd.read_csv(\"ball landmarks.csv\")\n",
    "\n",
    "# generating bootstrap samples\n",
    "X_var = mbelm_pred(landmark,convex_x)\n",
    "Y_var = mbelm_pred(landmark,convex_y)\n",
    "Z_var = mbelm_pred(landmark,convex_z)\n",
    "var_X = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(X_var[q])),size=1000) for q in range(len(X_var))])\n",
    "var_Y = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Y_var[q])),size=1000) for q in range(len(Y_var))])\n",
    "var_Z = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Z_var[q])),size=1000) for q in range(len(Z_var))])\n",
    "boot_sample = []\n",
    "for j in range(1000):\n",
    "    this_loop = pd.DataFrame({'X':landmark['X'].values+var_X[:,j],'Y':landmark['Y'].values+var_Y[:,j],'Z':landmark['Z'].values+var_Z[:,j],'nX':landmark['nX'].values,'nY':landmark['nY'].values,'nZ':landmark['nZ'].values})\n",
    "    boot_sample.append(this_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# one scan case\n",
    "scan_1_max_dev = []\n",
    "scan_1_mean_dev = []\n",
    "\n",
    "for boot_length in range(1000):\n",
    "    selection = np.random.randint(0,1000)\n",
    "    mean = landmark\n",
    "    reg_conf,_ = ppa([boot_sample[selection].copy(),mean.copy()])\n",
    "    dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "    devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "    scan_1_max_dev.append(np.max(devs))\n",
    "    scan_1_mean_dev.append(np.mean(devs))\n",
    "\n",
    "quantiles = pd.DataFrame({\"Maximum Deviation\":scan_1_max_dev,\"Mean Deviation\":scan_1_mean_dev})\n",
    "pickling_on = open(\"ball result/Ball \" + str(1) + \" scans.pickle\",\"wb\")\n",
    "pickle.dump(quantiles, pickling_on)\n",
    "pickling_on.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~ Scan 2 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 3 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 4 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 5 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 6 ~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# multiple scan case\n",
    "size = list(range(2,7))\n",
    "\n",
    "for s in size:\n",
    "    print(\"~~~~~~~~~~~~ Scan\",s,\"~~~~~~~~~~~~~~\")\n",
    "    scan_max_dev = []\n",
    "    scan_mean_dev = []\n",
    "    combs = []\n",
    "    for i in range(2000):\n",
    "        this_l = []\n",
    "        for j in range(s):\n",
    "            this_l.append(np.random.randint(0,1000))\n",
    "        combs.append(this_l)\n",
    "    choice = np.random.choice(list(range(len(combs))),size=1000,replace=False)\n",
    "\n",
    "    for c in range(len(choice)):\n",
    "        \n",
    "        this_boot_landmarks = [boot_sample[i] for i in combs[choice[c]]]\n",
    "        _, mean_conf = ppa(this_boot_landmarks)\n",
    "        mean = landmark\n",
    "        reg_conf,_ = ppa([mean_conf.copy(),mean.copy()])\n",
    "        dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "        devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "        scan_max_dev.append(np.max(devs))\n",
    "        scan_mean_dev.append(np.mean(devs))\n",
    "        \n",
    "    results = pd.DataFrame({\"Maximum Deviation\":scan_max_dev,\"Mean Deviation\":scan_mean_dev})\n",
    "    pickling_on = open(\"ball result/Ball \" + str(s) + \" scans.pickle\",\"wb\")\n",
    "    pickle.dump(results, pickling_on)\n",
    "    pickling_on.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### convex ###########\n",
    "landmark = pd.read_csv(\"convex landmarks.csv\")\n",
    "\n",
    "# generating bootstrap samples\n",
    "X_var = mbelm_pred(landmark,freeform_x)\n",
    "Y_var = mbelm_pred(landmark,freeform_y)\n",
    "Z_var = mbelm_pred(landmark,freeform_z)\n",
    "var_X = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(X_var[q])),size=1000) for q in range(len(X_var))])\n",
    "var_Y = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Y_var[q])),size=1000) for q in range(len(Y_var))])\n",
    "var_Z = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Z_var[q])),size=1000) for q in range(len(Z_var))])\n",
    "boot_sample = []\n",
    "for j in range(1000):\n",
    "    this_loop = pd.DataFrame({'X':landmark['X'].values+var_X[:,j],'Y':landmark['Y'].values+var_Y[:,j],'Z':landmark['Z'].values+var_Z[:,j],'nX':landmark['nX'].values,'nY':landmark['nY'].values,'nZ':landmark['nZ'].values})\n",
    "    boot_sample.append(this_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~ Scan 2 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 3 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 4 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 5 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 6 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 7 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 8 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 9 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 10 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 11 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 12 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 13 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 14 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 15 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 16 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 17 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 18 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 19 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 20 ~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# one scan case\n",
    "scan_1_max_dev = []\n",
    "scan_1_mean_dev = []\n",
    "\n",
    "for boot_length in range(1000):\n",
    "    selection = np.random.randint(0,1000)\n",
    "    mean = landmark\n",
    "    reg_conf,_ = ppa([boot_sample[selection].copy(),mean.copy()])\n",
    "    dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "    devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "    scan_1_max_dev.append(np.max(devs))\n",
    "    scan_1_mean_dev.append(np.mean(devs))\n",
    "\n",
    "quantiles = pd.DataFrame({\"Maximum Deviation\":scan_1_max_dev,\"Mean Deviation\":scan_1_mean_dev})\n",
    "pickling_on = open(\"convex result/Convex \" + str(1) + \" scans.pickle\",\"wb\")\n",
    "pickle.dump(quantiles, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# multiple scan case\n",
    "size = list(range(2,21))\n",
    "\n",
    "for s in size:\n",
    "    print(\"~~~~~~~~~~~~ Scan\",s,\"~~~~~~~~~~~~~~\")\n",
    "    scan_max_dev = []\n",
    "    scan_mean_dev = []\n",
    "    combs = []\n",
    "    for i in range(2000):\n",
    "        this_l = []\n",
    "        for j in range(s):\n",
    "            this_l.append(np.random.randint(0,1000))\n",
    "        combs.append(this_l)\n",
    "    choice = np.random.choice(list(range(len(combs))),size=1000,replace=False)\n",
    "\n",
    "    for c in range(len(choice)):\n",
    "        \n",
    "        this_boot_landmarks = [boot_sample[i] for i in combs[choice[c]]]\n",
    "        _, mean_conf = ppa(this_boot_landmarks)\n",
    "        mean = landmark\n",
    "        reg_conf,_ = ppa([mean_conf.copy(),mean.copy()])\n",
    "        dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "        devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "        scan_max_dev.append(np.max(devs))\n",
    "        scan_mean_dev.append(np.mean(devs))\n",
    "        \n",
    "    results = pd.DataFrame({\"Maximum Deviation\":scan_max_dev,\"Mean Deviation\":scan_mean_dev})\n",
    "    pickling_on = open(\"convex result/Convex \" + str(s) + \" scans.pickle\",\"wb\")\n",
    "    pickle.dump(results, pickling_on)\n",
    "    pickling_on.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### freeform ###########\n",
    "landmark = pd.read_csv(\"freeform landmarks.csv\")\n",
    "\n",
    "# generating bootstrap samples\n",
    "X_var = mbelm_pred(landmark,convex_x)\n",
    "Y_var = mbelm_pred(landmark,convex_y)\n",
    "Z_var = mbelm_pred(landmark,convex_z)\n",
    "var_X = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(X_var[q])),size=1000) for q in range(len(X_var))])\n",
    "var_Y = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Y_var[q])),size=1000) for q in range(len(Y_var))])\n",
    "var_Z = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Z_var[q])),size=1000) for q in range(len(Z_var))])\n",
    "boot_sample = []\n",
    "for j in range(1000):\n",
    "    this_loop = pd.DataFrame({'X':landmark['X'].values+var_X[:,j],'Y':landmark['Y'].values+var_Y[:,j],'Z':landmark['Z'].values+var_Z[:,j],'nX':landmark['nX'].values,'nY':landmark['nY'].values,'nZ':landmark['nZ'].values})\n",
    "    boot_sample.append(this_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~ Scan 2 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 3 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 4 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 5 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 6 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 7 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 8 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 9 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 10 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 11 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 12 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 13 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 14 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 15 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 16 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 17 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 18 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 19 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 20 ~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# one scan case\n",
    "scan_1_max_dev = []\n",
    "scan_1_mean_dev = []\n",
    "\n",
    "for boot_length in range(1000):\n",
    "    selection = np.random.randint(0,1000)\n",
    "    mean = landmark\n",
    "    reg_conf,_ = ppa([boot_sample[selection].copy(),mean.copy()])\n",
    "    dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "    devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "    scan_1_max_dev.append(np.max(devs))\n",
    "    scan_1_mean_dev.append(np.mean(devs))\n",
    "\n",
    "quantiles = pd.DataFrame({\"Maximum Deviation\":scan_1_max_dev,\"Mean Deviation\":scan_1_mean_dev})\n",
    "pickling_on = open(\"freeform result/Freeform \" + str(1) + \" scans.pickle\",\"wb\")\n",
    "pickle.dump(quantiles, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# multiple scan case\n",
    "size = list(range(2,21))\n",
    "\n",
    "for s in size:\n",
    "    print(\"~~~~~~~~~~~~ Scan\",s,\"~~~~~~~~~~~~~~\")\n",
    "    scan_max_dev = []\n",
    "    scan_mean_dev = []\n",
    "    combs = []\n",
    "    for i in range(2000):\n",
    "        this_l = []\n",
    "        for j in range(s):\n",
    "            this_l.append(np.random.randint(0,1000))\n",
    "        combs.append(this_l)\n",
    "    choice = np.random.choice(list(range(len(combs))),size=1000,replace=False)\n",
    "\n",
    "    for c in range(len(choice)):\n",
    "        \n",
    "        this_boot_landmarks = [boot_sample[i] for i in combs[choice[c]]]\n",
    "        _, mean_conf = ppa(this_boot_landmarks)\n",
    "        mean = landmark\n",
    "        reg_conf,_ = ppa([mean_conf.copy(),mean.copy()])\n",
    "        dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "        devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "        scan_max_dev.append(np.max(devs))\n",
    "        scan_mean_dev.append(np.mean(devs))\n",
    "        \n",
    "    results = pd.DataFrame({\"Maximum Deviation\":scan_max_dev,\"Mean Deviation\":scan_mean_dev})\n",
    "    pickling_on = open(\"freeform result/Freeform \" + str(s) + \" scans.pickle\",\"wb\")\n",
    "    pickle.dump(results, pickling_on)\n",
    "    pickling_on.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### freeform 2 ###########\n",
    "landmark = pd.read_csv(\"freeform 2 landmarks.csv\")\n",
    "\n",
    "# generating bootstrap samples\n",
    "X_var = mbelm_pred(landmark,convex_x)\n",
    "Y_var = mbelm_pred(landmark,convex_y)\n",
    "Z_var = mbelm_pred(landmark,convex_z)\n",
    "var_X = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(X_var[q])),size=1000) for q in range(len(X_var))])\n",
    "var_Y = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Y_var[q])),size=1000) for q in range(len(Y_var))])\n",
    "var_Z = np.array([np.random.normal(loc=0,scale=np.sqrt(np.abs(Z_var[q])),size=1000) for q in range(len(Z_var))])\n",
    "boot_sample = []\n",
    "for j in range(1000):\n",
    "    this_loop = pd.DataFrame({'X':landmark['X'].values+var_X[:,j],'Y':landmark['Y'].values+var_Y[:,j],'Z':landmark['Z'].values+var_Z[:,j],'nX':landmark['nX'].values,'nY':landmark['nY'].values,'nZ':landmark['nZ'].values})\n",
    "    boot_sample.append(this_loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~ Scan 2 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 3 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 4 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 5 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 6 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 7 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 8 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 9 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 10 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 11 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 12 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 13 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 14 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 15 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 16 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 17 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 18 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 19 ~~~~~~~~~~~~~~\n",
      "~~~~~~~~~~~~ Scan 20 ~~~~~~~~~~~~~~\n"
     ]
    }
   ],
   "source": [
    "# one scan case\n",
    "scan_1_max_dev = []\n",
    "scan_1_mean_dev = []\n",
    "\n",
    "for boot_length in range(1000):\n",
    "    selection = np.random.randint(0,1000)\n",
    "    mean = landmark\n",
    "    reg_conf,_ = ppa([boot_sample[selection].copy(),mean.copy()])\n",
    "    dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "    devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "    scan_1_max_dev.append(np.max(devs))\n",
    "    scan_1_mean_dev.append(np.mean(devs))\n",
    "\n",
    "quantiles = pd.DataFrame({\"Maximum Deviation\":scan_1_max_dev,\"Mean Deviation\":scan_1_mean_dev})\n",
    "pickling_on = open(\"freeform 2 result/Freeform 2 \" + str(1) + \" scans.pickle\",\"wb\")\n",
    "pickle.dump(quantiles, pickling_on)\n",
    "pickling_on.close()\n",
    "\n",
    "# multiple scan case\n",
    "size = list(range(2,21))\n",
    "\n",
    "for s in size:\n",
    "    print(\"~~~~~~~~~~~~ Scan\",s,\"~~~~~~~~~~~~~~\")\n",
    "    scan_max_dev = []\n",
    "    scan_mean_dev = []\n",
    "    combs = []\n",
    "    for i in range(2000):\n",
    "        this_l = []\n",
    "        for j in range(s):\n",
    "            this_l.append(np.random.randint(0,1000))\n",
    "        combs.append(this_l)\n",
    "    choice = np.random.choice(list(range(len(combs))),size=1000,replace=False)\n",
    "\n",
    "    for c in range(len(choice)):\n",
    "        \n",
    "        this_boot_landmarks = [boot_sample[i] for i in combs[choice[c]]]\n",
    "        _, mean_conf = ppa(this_boot_landmarks)\n",
    "        mean = landmark\n",
    "        reg_conf,_ = ppa([mean_conf.copy(),mean.copy()])\n",
    "        dev = reg_conf[0][['X','Y','Z']] - reg_conf[1][['X','Y','Z']]\n",
    "        devs = np.abs(np.sum(dev.values*reg_conf[1][['nX','nY','nZ']].values, axis=1))\n",
    "        scan_max_dev.append(np.max(devs))\n",
    "        scan_mean_dev.append(np.mean(devs))\n",
    "        \n",
    "    results = pd.DataFrame({\"Maximum Deviation\":scan_max_dev,\"Mean Deviation\":scan_mean_dev})\n",
    "    pickling_on = open(\"freeform 2 result/Freeform 2 \" + str(s) + \" scans.pickle\",\"wb\")\n",
    "    pickle.dump(results, pickling_on)\n",
    "    pickling_on.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
